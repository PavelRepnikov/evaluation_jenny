{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c05403-bf71-48a0-b72e-d24cbc025b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sentences have been saved to 'data/cleaned_sentences.csv'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to read text and split into sentences\n",
    "def extract_sentences_from_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split text into chunks if it exceeds the maximum length\n",
    "    max_length = nlp.max_length\n",
    "    sentences = []\n",
    "    \n",
    "    # Process the text in chunks\n",
    "    for start in range(0, len(text), max_length):\n",
    "        chunk = text[start:start + max_length]\n",
    "        doc = nlp(chunk)\n",
    "        sentences.extend([sent.text for sent in doc.sents])\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Function to clean sentences\n",
    "def clean_sentences(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Remove quotation marks\n",
    "        sentence = sentence.replace('“', '').replace('”', '').replace('\"', '')\n",
    "        \n",
    "        # Remove sentences like '1.E.7', chapter titles, or asterisks\n",
    "        if (not re.match(r'^\\d+\\.\\w+\\.\\d+$', sentence) and  # Matches patterns like '1.E.7'\n",
    "            not re.match(r'^CHAPTER \\d+$', sentence) and  # Matches 'CHAPTER 34'\n",
    "            not re.match(r'^\\*+$', sentence)):  # Matches lines with only asterisks\n",
    "            cleaned_sentences.append(sentence.strip())  # Strip whitespace\n",
    "    return cleaned_sentences\n",
    "\n",
    "# Example usage\n",
    "file_path = 'data/Moby Dick; Or, The Whale.txt'\n",
    "\n",
    "# Extract sentences\n",
    "sentences = extract_sentences_from_text(file_path)\n",
    "\n",
    "# Clean the extracted sentences\n",
    "cleaned_sentences = clean_sentences(sentences)\n",
    "\n",
    "# Optionally, you can save the cleaned sentences to a CSV file\n",
    "output_file = 'data/cleaned_sentences.csv'\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['sentences'])  # Write header\n",
    "    for sentence in cleaned_sentences:\n",
    "        writer.writerow([sentence])  # Write each cleaned sentence\n",
    "\n",
    "print(\"Cleaned sentences have been saved to 'data/cleaned_sentences.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e92289-2a78-4491-90bc-0be887c4b9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
